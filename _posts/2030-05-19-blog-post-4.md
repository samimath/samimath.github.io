---
title: 'Data Science Cheatsheet'
date: 2030-05-26
permalink: /posts/2020/05/blog-post-3/
tags:
  - API
  - data wrangling
---

{% include /head/custom.html %}

# Parameter estimation

## Overview
- estimating a statistic of interest, e.g. mean, variance, probability of certain events, etc

## Desirable properties:

- Consistent
  - Converges to true Parameter asymptotically

- Unbiased
 - Expected value of the parameter is equal to the true parameter

- Efficient
 - has lowest possible variance
 - indicates reliability

## Types of methods

### Parametric:


## Bias-Variance trade-off:

Model residual can be expressed into three parts:

1) Variance - characterized by the model
2) Noise - intrinsic to the data
3) Bias - inherent error from the model independent of sample size

E[ (h(x) - y)^2] = E[ (h(x) - \bar{y})^2] + E[ (\bar(y) - y)^2] + E[ (h(x) - \bar{y})^2]

**Regime 1 (High Variance)**

In the first regime, the cause of the poor performance is high variance.

- Symptoms:

  - Training error is much lower than test error
  - Training error is lower than ϵ
  - Test error is above ϵ

- Remedies:

  - Add more training data
  - Reduce model complexity -- complex models are prone to high variance
  - Bagging (will be covered later in the course)

Regime 2 (High Bias)
Unlike the first regime, the second regime indicates high bias: the model being used is not robust enough to produce an accurate prediction.
Symptoms:

Training error is higher than ϵ
Remedies:

Use more complex model (e.g. kernelize, use non-linear models)
Add features
Boosting (will be covered later in the course)


** Support vector machine**:
- constructs a hyperplane using support vectors. A good separation is achieved by a hyperplane that has the largest distance to the nearest training data point (functional margin). Support vectors are the points that are 'most difficult to classify' for their class.
- optimize margin between classes
- The 'Kernel' trick : a weighing function k(x,x') which can be used to separate the hyperplane in the data space, useful for non-linear classification


**Anomaly detection method**

- Kolmogorov-Smirnov test: compares max distance between observed CDF and theoretical CDF

**Functional data analysis**
- data smoothing & interpolation
- functional statistics : functional mean and variance


**t-test**
- compare population mean, has mean 0 under null hypothesis

**F test**
- compare group means, has mean 1 (ratio of two groups) under null hypothesis

**Chi squared test**
- tells us if the observed distribution within groups are significantly different than expected

**random effects model**
- group effects are random
